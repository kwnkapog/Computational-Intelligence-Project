{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from utils import train_model\n",
    "from utils import mean_of_all\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv('iphi2802.csv', delimiter='\\t')\n",
    "\n",
    "# Print the original shape of the dataset\n",
    "print(f\"Original shape of Dataset: {df.shape}\")\n",
    "\n",
    "# Print information about the dataset\n",
    "print(f'\\nDataset Information:')\n",
    "df.info()\n",
    "\n",
    "# Print the number of NULL values in each column\n",
    "print(f'\\nNumber of NULL values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Print the number of unique values in each column\n",
    "print(f'\\nNumber of unique values per column:')\n",
    "print(df.nunique())\n",
    "\n",
    "# Create a new column 'mean_date' in the dataframe, which is the mean of the two dates  \n",
    "df['mean_date'] = df[['date_min', 'date_max']].mean(axis=1)\n",
    "\n",
    "\n",
    "# Iitializing the tf-idf vectorizer, using a stopword list from the nltk library and and transform the 'text' column into a TF-IDF matrix of 1000 columns\n",
    "stopwords = nltk.corpus.stopwords.words('greek') \n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=8000)\n",
    "index_matrix = vectorizer.fit_transform(df['text'].to_list())\n",
    "\n",
    "# Visualize the output of the vectorizer(words and their idf values)\n",
    "shape = index_matrix.shape\n",
    "idf_values = vectorizer.idf_\n",
    "vocab = sorted(vectorizer.vocabulary_)\n",
    "\n",
    "# Convert the input/target martices for normalization\n",
    "texts = index_matrix.toarray()\n",
    "dates = df['mean_date'].values.reshape(-1,1)\n",
    "\n",
    "# Initialize a MinMaxScaler and scale both the TF-IDF matrix (input) and the mean_dates (output) column\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(texts)\n",
    "y = scaler.fit_transform(dates)\n",
    "\n",
    "# Initialize 5-Fold Cross Validation, create dictionary of each fold, store all dictionaries to fold_dataset list\n",
    "fold_dataset = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    \n",
    "    # split data to train/test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Store the training and test datasets along with fold index\n",
    "    fold_data = {\n",
    "        \"fold_index\": fold_index,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "    fold_dataset.append(fold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate various models \n",
    "mean_losses_tr = []\n",
    "mean_losses_ts = []\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model(250, 0.001, 0.9, fold_dataset, 32, 50, False, False, 0.0, 0.0)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([500,300], 0.001, fold_dataset, 32, 50, False, False, 0.0, 0.0)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([400,200,100], 0.001, fold_dataset, 32, 50, False, False, 0.0, 0.0)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([500,300,100], 0.001, fold_dataset, 32, 50, False, False, 0.0, 0.0)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([800,400,200], 0.001, fold_dataset, 32, 50, False, False, 0.0, 0.0)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each one individually\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (train_loss, test_loss) in enumerate(zip(mean_losses_tr, mean_losses_ts)):\n",
    "    plt.plot(train_loss, label=f'Mean Train Loss Network {i + 1}')\n",
    "    plt.plot(test_loss, linestyle='--', label=f'Mean Test Loss Network {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Mean Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the best model of the above using the early stopping criterion and plot the loss per fold for all the epochs.\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.001, fold_dataset, 32, 150, True, False, 0.0, 0.0)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs for every Fold with Early Stopping')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the effects of combinations of various learning rate and momentum values\n",
    "\n",
    "final_means = []\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.001, 0.2, fold_dataset, 32, 150, True, False, 0.0, 0.0)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Early Stopping, η=0.001, m=0.6')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.001, 0.6, fold_dataset, 32, 150,True, False, 0.0, 0.0)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Early Stopping, η=0.001, m=0.6')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.05, 0.6, fold_dataset, 32, 150, True, False, 0.0, 0.0)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Early Stopping, η=0.05, m=0.6')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.1, 0.6, fold_dataset, 32, 150, True, False, 0.0, 0.0)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Early Stopping, η=0.1, m=0.6')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network using dropout regularization with various values for dropout rates for the input and hidden layers \n",
    "final_means = []\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.001, 0.9, fold_dataset, 32, 150, have_callback=True, dropout=True, dropout_in=0.8, dropout_h=0.5)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Dropout regularization, r_in=0.8, r_h= 0.5')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.001, 0.9, fold_dataset, 32, 150, have_callback=True, dropout=True, dropout_in=0.5, dropout_h=0.5)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Dropout regularization, r_in=0.5, r_h= 0.5')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "train_loss, test_loss = train_model([500,300,100], 0.05, 0.9, fold_dataset, 32, 150, have_callback=True, dropout=True, dropout_in=0.8, dropout_h=0.2)\n",
    "final_means.append(mean_of_all(train_loss))\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (tr_loss, ts_loss) in enumerate(zip(train_loss, test_loss)):\n",
    "    plt.plot(tr_loss, label=f'Train Loss Fold {i + 1}')\n",
    "    plt.plot(ts_loss, linestyle='--', label=f'Test Loss Fold {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Loss Over Epochs per Fold with Dropout regularization, r_in=0.8, r_h= 0.2')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computIntel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
