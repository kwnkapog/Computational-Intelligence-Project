{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import neural_network \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from neural_network import train_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv('iphi2802.csv', delimiter='\\t')\n",
    "\n",
    "# Print the original shape of the dataset\n",
    "print(f\"Original shape of Dataset: {df.shape}\")\n",
    "\n",
    "# Print information about the dataset\n",
    "print(f'\\nDataset Information:')\n",
    "df.info()\n",
    "\n",
    "# Print the number of NULL values in each column\n",
    "print(f'\\nNumber of NULL values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Print the number of unique values in each column\n",
    "print(f'\\nNumber of unique values per column:')\n",
    "print(df.nunique())\n",
    "\n",
    "# Create a new column 'mean_date' in the dataframe, which is the mean of the two dates  \n",
    "df['mean_date'] = df[['date_min', 'date_max']].mean(axis=1)\n",
    "\n",
    "\n",
    "# Iitializing the tf-idf vectorizer, using a stopword list from the nltk library and and transform the 'text' column into a TF-IDF matrix of 1000 columns\n",
    "stopwords = nltk.corpus.stopwords.words('greek') \n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=8000)\n",
    "index_matrix = vectorizer.fit_transform(df['text'].to_list())\n",
    "\n",
    "# Visualize the output of the vectorizer(words and their idf values)\n",
    "shape = index_matrix.shape\n",
    "idf_values = vectorizer.idf_\n",
    "vocab = sorted(vectorizer.vocabulary_)\n",
    "\n",
    "# Convert the input/target martices for normalization\n",
    "texts = index_matrix.toarray()\n",
    "dates = df['mean_date'].values.reshape(-1,1)\n",
    "\n",
    "# Initialize a MinMaxScaler and scale both the TF-IDF matrix (input) and the mean_dates (output) column\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(texts)\n",
    "y = scaler.fit_transform(dates)\n",
    "\n",
    "# Initialize 5-Fold Cross Validation, create dictionary of each fold, store all dictionaries to fold_dataset list\n",
    "fold_dataset = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    \n",
    "    # split data to train/test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Store the training and test datasets along with fold index\n",
    "    fold_data = {\n",
    "        \"fold_index\": fold_index,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "    fold_dataset.append(fold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models of 1 hidden layer\n",
    "\n",
    "mean_train_losses = []\n",
    "tr_loss1, ts_loss1, mean_tr_loss1 = train_model(1000, 0.001, True, fold_dataset, 32, 50)\n",
    "mean_train_losses.append(mean_tr_loss1)\n",
    "tr_loss2, ts_loss2, mean_tr_loss2 = train_model(750, 0.001, True, fold_dataset, 32, 50)\n",
    "mean_train_losses.append(mean_tr_loss2)\n",
    "tr_loss3, ts_loss3, mean_tr_loss3 = train_model(500, 0.001, True, fold_dataset, 32, 50)\n",
    "mean_train_losses.append(mean_tr_loss3)\n",
    "tr_loss4, ts_loss4, mean_tr_loss4 = train_model(250, 0.001, True, fold_dataset, 32, 50)\n",
    "mean_train_losses.append(mean_tr_loss4)\n",
    "\n",
    "# Plot the mean train losses over epochs and test losses over folds for the models\n",
    "plt.figure(figsize=(12, 4))\n",
    " \n",
    "for i,loss in enumerate(mean_train_losses):\n",
    "    plt.plot(loss, label='Mean Train Loss Network {}'.format(i+1))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Mean Loss Over Epochs')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.plot(ts_loss1, label='Test Loss Network 1')\n",
    "plt.plot(ts_loss2, label='Test Loss Network 2')\n",
    "plt.plot(ts_loss3, label='Test Loss Network 3')\n",
    "plt.plot(ts_loss4, label='Test Loss Network 4')\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Train Loss Over Folds per Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models for two hidden layers\n",
    "mean_loss_ml = []\n",
    "\n",
    "tr_loss_ml_1, ts_loss_ml_1, mean_tr_loss_ml_1 = train_model(\n",
    "    [250,50], 0.001, True, fold_dataset, 32, 50)\n",
    "mean_loss_ml.append(mean_tr_loss_ml_1)\n",
    "tr_loss_ml_2, ts_loss_ml_2, mean_tr_loss_ml_2 = train_model(\n",
    "    [400,200], 0.001, True, fold_dataset, 32, 50)\n",
    "mean_loss_ml.append(mean_tr_loss_ml_2)\n",
    "tr_loss_ml_3, ts_loss_ml_3, mean_tr_loss_ml_3 = train_model(\n",
    "    [500,300], 0.001, True, fold_dataset, 32, 50)\n",
    "mean_loss_ml.append(mean_tr_loss_ml_3)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    " \n",
    "for i,loss in enumerate(mean_loss_ml):\n",
    "    plt.plot(loss, label='Mean Train Loss Multilayered Network {}'.format(i+1))\n",
    "\n",
    "plt.legend()\n",
    "plt.title('Mean Loss Over Epochs')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(ts_loss_ml_1, label='Test Loss Network 1')\n",
    "plt.plot(ts_loss_ml_2, label='Test Loss Network 2')\n",
    "plt.plot(ts_loss_ml_3, label='Test Loss Network 3')\n",
    "plt.legend()\n",
    "plt.title('Train Loss Over Folds per Model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computIntel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
