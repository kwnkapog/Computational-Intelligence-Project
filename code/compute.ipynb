{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import neural_network \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from neural_network import train_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe.\n",
    "df = pd.read_csv('iphi2802.csv', delimiter='\\t')\n",
    "\n",
    "# Print the original shape of the dataset\n",
    "print(f\"Original shape of Dataset: {df.shape}\")\n",
    "\n",
    "# Print information about the dataset\n",
    "print(f'\\nDataset Information:')\n",
    "df.info()\n",
    "\n",
    "# Print the number of NULL values in each column\n",
    "print(f'\\nNumber of NULL values per column:')\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Print the number of unique values in each column\n",
    "print(f'\\nNumber of unique values per column:')\n",
    "print(df.nunique())\n",
    "\n",
    "# Create a new column 'mean_date' in the dataframe, which is the mean of the two dates  \n",
    "df['mean_date'] = df[['date_min', 'date_max']].mean(axis=1)\n",
    "\n",
    "\n",
    "# Iitializing the tf-idf vectorizer, using a stopword list from the nltk library and and transform the 'text' column into a TF-IDF matrix of 1000 columns\n",
    "stopwords = nltk.corpus.stopwords.words('greek') \n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, max_features=8000)\n",
    "index_matrix = vectorizer.fit_transform(df['text'].to_list())\n",
    "\n",
    "# Visualize the output of the vectorizer(words and their idf values)\n",
    "shape = index_matrix.shape\n",
    "idf_values = vectorizer.idf_\n",
    "vocab = sorted(vectorizer.vocabulary_)\n",
    "\n",
    "# Convert the input/target martices for normalization\n",
    "texts = index_matrix.toarray()\n",
    "dates = df['mean_date'].values.reshape(-1,1)\n",
    "\n",
    "# Initialize a MinMaxScaler and scale both the TF-IDF matrix (input) and the mean_dates (output) column\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(texts)\n",
    "y = scaler.fit_transform(dates)\n",
    "\n",
    "# Initialize 5-Fold Cross Validation, create dictionary of each fold, store all dictionaries to fold_dataset list\n",
    "fold_dataset = []\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold_index, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "    \n",
    "    # split data to train/test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Store the training and test datasets along with fold index\n",
    "    fold_data = {\n",
    "        \"fold_index\": fold_index,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_test\": X_test,\n",
    "        \"y_test\": y_test,\n",
    "    }\n",
    "    fold_dataset.append(fold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate various models \n",
    "mean_losses_tr = []\n",
    "mean_losses_ts = []\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model(250, 0.001, pr.fold_dataset, 32, 50)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([500,300], 0.001, pr.fold_dataset, 32, 50)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([400,200,100], 0.001, pr.fold_dataset, 32, 50)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([500,300,100], 0.001, pr.fold_dataset, 32, 50)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n",
    "mean_train_loss, mean_test_loss = train_model([800,400,200], 0.001, pr.fold_dataset, 32, 50)\n",
    "mean_losses_tr.append(mean_train_loss)\n",
    "mean_losses_ts.append(mean_test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation losses for each one individually\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for i, (train_loss, test_loss) in enumerate(zip(mean_losses_tr, mean_losses_ts)):\n",
    "    plt.plot(train_loss, label=f'Mean Train Loss Network {i + 1}')\n",
    "    plt.plot(test_loss, linestyle='--', label=f'Mean Test Loss Network {i + 1}')\n",
    "\n",
    "plt.legend(fontsize = 6)\n",
    "plt.title('Mean Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computIntel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
